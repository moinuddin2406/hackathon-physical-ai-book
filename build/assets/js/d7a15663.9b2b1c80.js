"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_course=globalThis.webpackChunkphysical_ai_humanoid_robotics_course||[]).push([[894],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var t=i(6540);const a={},s=t.createContext(a);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},9155:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var t=i(4848),a=i(8453);const s={title:"Vision-Language-Action (VLA) Systems",sidebar_label:"Vision-Language-Action Systems"},o="Vision-Language-Action (VLA) Systems",r={id:"module4/vision-language-action-vla",title:"Vision-Language-Action (VLA) Systems",description:"Introduction",source:"@site/docs/module4/vision-language-action-vla.md",sourceDirName:"module4",slug:"/module4/vision-language-action-vla",permalink:"/hackathon-physical-ai-book/docs/module4/vision-language-action-vla",draft:!1,unlisted:!1,editUrl:"https://github.com/moinuddin2406/hackathon-physical-ai-book/edit/main/docs/module4/vision-language-action-vla.md",tags:[],version:"current",frontMatter:{title:"Vision-Language-Action (VLA) Systems",sidebar_label:"Vision-Language-Action Systems"},sidebar:"docs",previous:{title:"AI-Robot Integration",permalink:"/hackathon-physical-ai-book/docs/module3/ai-robot-brain-nvidia-isaac"},next:{title:"RAG Chat Interface",permalink:"/hackathon-physical-ai-book/docs/rag-interface"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Understanding VLA Systems",id:"understanding-vla-systems",level:2},{value:"Core Components of VLA Systems",id:"core-components-of-vla-systems",level:3},{value:"VLA Architecture Types",id:"vla-architecture-types",level:3},{value:"Vision Processing in VLA Systems",id:"vision-processing-in-vla-systems",level:2},{value:"Visual Representations for Action",id:"visual-representations-for-action",level:3},{value:"Language Understanding in VLA Systems",id:"language-understanding-in-vla-systems",level:2},{value:"Language Modalities",id:"language-modalities",level:3},{value:"Grounding Language in Perception",id:"grounding-language-in-perception",level:3},{value:"Action Representation and Selection",id:"action-representation-and-selection",level:2},{value:"Action Space Considerations",id:"action-space-considerations",level:3},{value:"VLA Training Approaches",id:"vla-training-approaches",level:2},{value:"Supervised Learning",id:"supervised-learning",level:3},{value:"Reinforcement Learning",id:"reinforcement-learning",level:3},{value:"Imitation Learning",id:"imitation-learning",level:3},{value:"Pre-training and Fine-tuning",id:"pre-training-and-fine-tuning",level:3},{value:"Implementing VLA Systems",id:"implementing-vla-systems",level:2},{value:"Data Requirements",id:"data-requirements",level:3},{value:"Model Architecture Example",id:"model-architecture-example",level:3},{value:"Example 3: Complete VLA System with Robot Interface",id:"example-3-complete-vla-system-with-robot-interface",level:3},{value:"Next Steps",id:"next-steps",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"vision-language-action-vla-systems",children:"Vision-Language-Action (VLA) Systems"}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems represent the cutting edge of artificial intelligence in robotics, where visual perception, language understanding, and physical action are tightly integrated into unified intelligent systems. VLA models process visual input, understand natural language commands, and generate appropriate motor actions to complete complex tasks. This technology enables robots to interact with humans using natural language while perceiving and manipulating objects in their environment."}),"\n",(0,t.jsx)(n.p,{children:"VLA systems have emerged as a powerful paradigm that addresses the challenge of grounding high-level language commands in low-level sensory-motor actions. Rather than treating perception, language, and action as separate modules, VLA systems learn the relationships between these modalities end-to-end, enabling more natural and flexible human-robot interaction."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the architecture and components of Vision-Language-Action systems"}),"\n",(0,t.jsx)(n.li,{children:"Implement visual perception and language understanding for robotic tasks"}),"\n",(0,t.jsx)(n.li,{children:"Design action selection and execution modules for VLA systems"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate the performance of VLA models in robotic manipulation tasks"}),"\n",(0,t.jsx)(n.li,{children:"Address challenges in training and deploying VLA systems"}),"\n",(0,t.jsx)(n.li,{children:"Integrate VLA systems with existing robot platforms"}),"\n",(0,t.jsx)(n.li,{children:"Recognize limitations and future directions in VLA research"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"understanding-vla-systems",children:"Understanding VLA Systems"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action systems are multimodal AI architectures that jointly process visual, linguistic, and motor information. These systems differ from traditional robotics approaches that handle these modalities separately. Instead of first detecting objects in a scene, then parsing a linguistic command, and finally planning a manipulation action, VLA systems learn correlations between all three modalities simultaneously."}),"\n",(0,t.jsx)(n.h3,{id:"core-components-of-vla-systems",children:"Core Components of VLA Systems"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Encoder"}),": Processes visual input (images, videos, 3D point clouds) to extract relevant features"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Encoder"}),": Processes natural language commands to extract semantic meaning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Decoder"}),": Maps the combined visual and language representation to action parameters"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fusion Module"}),": Integrates information from different modalities"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"vla-architecture-types",children:"VLA Architecture Types"}),"\n",(0,t.jsx)(n.p,{children:"VLA systems can be categorized based on their architectural approach:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Separate Encoders with Late Fusion"}),": Each modality has dedicated encoders that are fused at a later stage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified Multimodal Encoder"}),": A single encoder processes all modalities simultaneously"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hierarchical Approaches"}),": Multiple levels of abstraction with different fusion strategies at each level"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    A["Image/Video"] --\x3e B["Visual Encoder"]\n    C["Natural Language"] --\x3e D["Language Encoder"]\n    B --\x3e E["Multimodal Fusion"]\n    D --\x3e E\n    E --\x3e F["Action Decoder"]\n    F --\x3e G["Motor Actions"]\n    \n    style A fill:#cde4ff\n    style C fill:#cde4ff\n    style B fill:#f9f\n    style D fill:#f9f\n    style E fill:#f9f\n    style F fill:#f9f\n    style G fill:#ffcdcd\n'})}),"\n",(0,t.jsx)(n.h2,{id:"vision-processing-in-vla-systems",children:"Vision Processing in VLA Systems"}),"\n",(0,t.jsx)(n.p,{children:"The vision component of VLA systems is responsible for extracting relevant information from visual input. This may include object detection, segmentation, spatial relationships, or more abstract visual concepts that are relevant to the task."}),"\n",(0,t.jsx)(n.p,{children:"Modern VLA systems often use convolutional neural networks (CNNs) or vision transformers (ViTs) as visual encoders. These models are typically pre-trained on large visual datasets and fine-tuned for the specific robotics task. The visual encoder outputs a feature representation that captures important visual information while discarding irrelevant details."}),"\n",(0,t.jsx)(n.h3,{id:"visual-representations-for-action",children:"Visual Representations for Action"}),"\n",(0,t.jsx)(n.p,{children:"In VLA systems, the visual representation must be suitable for action selection. This means the system needs to identify:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Reachable objects and their affordances"}),"\n",(0,t.jsx)(n.li,{children:"Spatial relationships between objects and the robot"}),"\n",(0,t.jsx)(n.li,{children:"Scene context that may inform action selection"}),"\n",(0,t.jsx)(n.li,{children:"Dynamic elements that require special attention"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"language-understanding-in-vla-systems",children:"Language Understanding in VLA Systems"}),"\n",(0,t.jsx)(n.p,{children:"The language component processes natural language commands and instructions. In VLA systems, language understanding goes beyond mere parsing to focus on identifying the task-relevant semantic content that informs action selection."}),"\n",(0,t.jsx)(n.h3,{id:"language-modalities",children:"Language Modalities"}),"\n",(0,t.jsx)(n.p,{children:"VLA systems handle various forms of linguistic input:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High-level Commands"}),': "Pick up the red cup" or "Move the box to the left"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Goal Descriptions"}),': "Set the table for dinner"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Demonstrative Instructions"}),': "Do what I just showed you"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Abstract Goals"}),': "Prepare a snack" (requiring task decomposition)']}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"grounding-language-in-perception",children:"Grounding Language in Perception"}),"\n",(0,t.jsx)(n.p,{children:'A key challenge in VLA systems is grounding language in perceptual inputs. This means associating linguistic references ("the red cup") with visual objects in the scene. The system must resolve ambiguity, handle novel combinations of concepts, and understand spatial relationships expressed in language.'}),"\n",(0,t.jsx)(n.h2,{id:"action-representation-and-selection",children:"Action Representation and Selection"}),"\n",(0,t.jsx)(n.p,{children:"The action component of VLA systems translates the multimodal understanding into executable robot actions. These actions can be represented in various forms:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"End-effector Pose"}),": Cartesian coordinates and orientation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Joint Angles"}),": Direct joint-level commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Motion Primitives"}),": Learned movement patterns"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Discrete Actions"}),": Pick/place, open/close, etc."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"action-space-considerations",children:"Action Space Considerations"}),"\n",(0,t.jsx)(n.p,{children:"The action space in VLA systems needs to be:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Expressive"}),": Capable of representing the full range of intended behaviors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Generalizable"}),": Able to handle objects and situations not seen during training"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safe"}),": Incorporating safety constraints and fail-safes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Efficient"}),": Allowing for rapid action selection during deployment"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"vla-training-approaches",children:"VLA Training Approaches"}),"\n",(0,t.jsx)(n.p,{children:"Training VLA systems typically requires large datasets of visual, linguistic, and action demonstrations. Several approaches exist:"}),"\n",(0,t.jsx)(n.h3,{id:"supervised-learning",children:"Supervised Learning"}),"\n",(0,t.jsx)(n.p,{children:"Using datasets of human demonstrations that include visual observations, language annotations, and recorded actions. The model learns to predict actions given visual and linguistic inputs."}),"\n",(0,t.jsx)(n.h3,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,t.jsx)(n.p,{children:"Training the system through trial-and-error interaction with the environment, guided by rewards that reflect task success. This approach can learn complex behaviors but requires extensive interaction."}),"\n",(0,t.jsx)(n.h3,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,t.jsx)(n.p,{children:"Learning from expert demonstrations by mimicking the observed behavior. This approach is often combined with other techniques for better performance."}),"\n",(0,t.jsx)(n.h3,{id:"pre-training-and-fine-tuning",children:"Pre-training and Fine-tuning"}),"\n",(0,t.jsx)(n.p,{children:"Leveraging large pre-trained vision and language models, then fine-tuning them on robotics-specific data to reduce the need for extensive task-specific training data."}),"\n",(0,t.jsx)(n.h2,{id:"implementing-vla-systems",children:"Implementing VLA Systems"}),"\n",(0,t.jsx)(n.h3,{id:"data-requirements",children:"Data Requirements"}),"\n",(0,t.jsx)(n.p,{children:"VLA systems require datasets with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Data"}),": Images/video of the robot and environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Annotations"}),": Natural language commands corresponding to actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Labels"}),": Actual actions taken in response to the visual and language inputs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temporal Information"}),": Sequences of states and actions for dynamic tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"model-architecture-example",children:"Model Architecture Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass VLAModel(nn.Module):\n    def __init__(self, vocab_size, action_dim, hidden_dim=512):\n        super(VLAModel, self).__init__()\n        \n        # Visual encoder (e.g., ResNet or Vision Transformer)\n        self.visual_encoder = models.resnet18(pretrained=True)\n        # Remove the final classification layer\n        self.visual_encoder.fc = nn.Identity()\n        \n        # Language encoder\n        self.lang_embed = nn.Embedding(vocab_size, hidden_dim)\n        self.lang_lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n        \n        # Fusion module\n        self.fusion = nn.Linear(2 * hidden_dim, hidden_dim)\n        \n        # Action decoder\n        self.action_head = nn.Linear(hidden_dim, action_dim)\n        \n    def forward(self, visual_input, lang_input):\n        # Encode visual features\n        vis_features = self.visual_encoder(visual_input)\n        \n        # Encode language features\n        lang_embed = self.lang_embed(lang_input)\n        lang_features, _ = self.lang_lstm(lang_embed)\n        # Take the last output\n        lang_features = lang_features[:, -1, :]\n        \n        # Concatenate and fuse modalities\n        combined = torch.cat([vis_features, lang_features], dim=1)\n        fused = torch.relu(self.fusion(combined))\n        \n        # Decode to action space\n        actions = self.action_head(fused)\n        \n        return actions\n\n# Example usage\nvla_model = VLAModel(vocab_size=10000, action_dim=7)  # 7-DOF robot arm\n\n### Example 2: VLA System with CLIP Integration\n```python\nimport torch\nimport torch.nn as nn\nimport clip\nfrom torchvision import transforms\n\nclass CLIPBasedVLA(nn.Module):\n    def __init__(self, action_space_dim, clip_model_name="ViT-B/32"):\n        super(CLIPBasedVLA, self).__init__()\n\n        # Load pre-trained CLIP model\n        self.clip_model, self.preprocess = clip.load(clip_model_name)\n\n        # Freeze CLIP parameters (optional - for fine-tuning later)\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n\n        # Projection layers to map CLIP features to action space\n        self.visual_projection = nn.Sequential(\n            nn.Linear(512, 256),  # Adjust input size based on CLIP model\n            nn.ReLU(),\n            nn.Linear(256, 128)\n        )\n\n        self.lang_projection = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128)\n        )\n\n        # Action prediction head\n        self.action_head = nn.Sequential(\n            nn.Linear(256, 128),  # Combined size of visual and lang features\n            nn.ReLU(),\n            nn.Linear(128, action_space_dim)\n        )\n\n    def encode_visual(self, images):\n        # Process images through CLIP visual encoder\n        image_features = self.clip_model.encode_image(images)\n        # Normalize features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        # Project to action-relevant space\n        projected_features = self.visual_projection(image_features)\n        return projected_features\n\n    def encode_language(self, text):\n        # Process text through CLIP text encoder\n        text_features = self.clip_model.encode_text(text)\n        # Normalize features\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        # Project to action-relevant space\n        projected_features = self.lang_projection(text_features)\n        return projected_features\n\n    def forward(self, images, text):\n        # Encode visual and language inputs\n        vis_features = self.encode_visual(images)\n        lang_features = self.encode_language(text)\n\n        # Concatenate features\n        combined_features = torch.cat([vis_features, lang_features], dim=1)\n\n        # Predict actions\n        actions = self.action_head(combined_features)\n\n        return actions\n\ndef process_command_with_vla(vla_model, image, command_text, device=\'cpu\'):\n    """\n    Process a visual input and natural language command with the VLA model\n    """\n    # Prepare image for CLIP\n    image_input = vla_model.preprocess(image).unsqueeze(0).to(device)\n\n    # Tokenize command text for CLIP\n    text_input = clip.tokenize([command_text]).to(device)\n\n    # Get action prediction\n    with torch.no_grad():\n        action_prediction = vla_model(image_input, text_input)\n\n    return action_prediction\n\n# Example usage\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nvla_model = CLIPBasedVLA(action_space_dim=7).to(device)  # 7-DOF robot arm\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-3-complete-vla-system-with-robot-interface",children:"Example 3: Complete VLA System with Robot Interface"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport numpy as np\nimport cv2\nfrom abc import ABC, abstractmethod\n\nclass RobotInterface(ABC):\n    """\n    Abstract interface for robot control\n    """\n    @abstractmethod\n    def get_observation(self):\n        """Get current robot state and sensor readings"""\n        pass\n\n    @abstractmethod\n    def execute_action(self, action):\n        """Execute an action in the environment"""\n        pass\n\n    @abstractmethod\n    def reset(self):\n        """Reset the robot to initial state"""\n        pass\n\nclass VLARobotController:\n    """\n    Complete VLA system that integrates vision, language, and action\n    """\n    def __init__(self, vla_model, robot_interface):\n        self.vla_model = vla_model\n        self.robot_interface = robot_interface\n        self.device = next(vla_model.parameters()).device\n\n    def execute_command(self, command_text, max_steps=100):\n        """\n        Execute a natural language command using VLA system\n        """\n        for step in range(max_steps):\n            # Get current observation (image + state)\n            obs = self.robot_interface.get_observation()\n\n            # Process with VLA model\n            action = self.vla_model(obs[\'image\'], obs[\'command_text\'])\n\n            # Execute action\n            self.robot_interface.execute_action(action)\n\n            # Check if task is complete (simplified)\n            if self.is_task_complete(obs, command_text):\n                print(f"Task completed in {step+1} steps")\n                break\n\n    def is_task_complete(self, obs, command_text):\n        """\n        Determine if the current task is complete based on observation and command\n        This is a simplified implementation - real implementation would be more sophisticated\n        """\n        # In a real implementation, this would use more sophisticated logic\n        # such as checking if a specific goal state is reached\n        return False\n\nclass SimpleRobotSimulator(RobotInterface):\n    """\n    Simple simulator implementing RobotInterface\n    """\n    def __init__(self):\n        self.position = [0.0, 0.0, 0.0]  # x, y, z\n        self.orientation = [0.0, 0.0, 0.0]  # roll, pitch, yaw\n        self.gripper_state = "open"  # "open" or "closed"\n\n    def get_observation(self):\n        """\n        Get observation including a simulated image and state\n        """\n        # In a real implementation, this would capture an actual image\n        # For simulation, we\'ll create a dummy image\n        dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n\n        # Add some simulated "objects" to make it more realistic\n        cv2.rectangle(dummy_image, (100, 100), (200, 200), (255, 0, 0), -1)  # Blue cube\n        cv2.circle(dummy_image, (300, 300), 50, (0, 255, 0), -1)  # Green circle\n\n        return {\n            \'image\': dummy_image,\n            \'state\': {\n                \'position\': self.position,\n                \'orientation\': self.orientation,\n                \'gripper_state\': self.gripper_state\n            }\n        }\n\n    def execute_action(self, action):\n        """\n        Execute a 7-DOF action (for a robot arm)\n        [dx, dy, dz, droll, dpitch, dyaw, gripper_action]\n        """\n        if len(action) >= 7:\n            # Update position\n            self.position[0] += action[0].item() * 0.01  # Scale down the action\n            self.position[1] += action[1].item() * 0.01\n            self.position[2] += action[2].item() * 0.01\n\n            # Update orientation\n            self.orientation[0] += action[3].item() * 0.01\n            self.orientation[1] += action[4].item() * 0.01\n            self.orientation[2] += action[5].item() * 0.01\n\n            # Handle gripper action\n            gripper_val = action[6].item()\n            if gripper_val > 0.5:\n                self.gripper_state = "closed"\n            elif gripper_val < -0.5:\n                self.gripper_state = "open"\n\n            print(f"Action executed: pos={self.position}, gripper={self.gripper_state}")\n\n    def reset(self):\n        """Reset the robot to initial state"""\n        self.position = [0.0, 0.0, 0.0]\n        self.orientation = [0.0, 0.0, 0.0]\n        self.gripper_state = "open"\n\n# Example usage of complete VLA system\ndef run_vla_example():\n    # Create models and interfaces (using dummy models for this example)\n    # vla_model = CLIPBasedVLA(action_space_dim=7)\n    robot_sim = SimpleRobotSimulator()\n\n    # Create VLA controller\n    # vla_controller = VLARobotController(vla_model, robot_sim)\n\n    # Execute a command\n    command = "pick up the blue cube"\n    # vla_controller.execute_command(command)\n\n    print("VLA System demo completed")\n\nif __name__ == "__main__":\n    run_vla_example()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Capstone project will be added in the final version."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);